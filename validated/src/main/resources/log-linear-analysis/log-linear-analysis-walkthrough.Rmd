---
title: "Log Linear Analysis Walkthrough"
author: "Tom Dalton"
date: "09/02/2017"
output: pdf_document
---

This walkthrough has been created in the process of following the tutorial here: https://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html

This document records the steps taken to perform a log linear analysis in R. The first section makes use of a toy data set before complicating things in the second section and using data from a genalogical population generator and looking to assert the similarity of the generators input parameters to the resulting population.

#Section 1 - Log linear analysis of Titanic data

##Preliminary introductions...

First, lets read the data in:
```{r}
data(Titanic)
dimnames(Titanic)
margin.table(Titanic)
```

***

Log linear analysis will allow us to look for relationships among the varibles in the multiway contingency table that we have. 

Log linear analysis assumes:

  * All observations in the tables are independant _(observations __not__ variables_)
  * Cell by cell frequency are sufficiently high _(generally 5 or more)_
  
***

To start with the basics, lets look at the how surivial varies with gender. Here we have the data:

```{r}
margin.table(Titanic, c(2,4))
```



```{r echo=FALSE}
male_survivors = margin.table(Titanic, c(2,4))[3]
male_deaths = margin.table(Titanic, c(2,4))[1]
total_males = male_deaths + male_survivors
female_survivors = margin.table(Titanic, c(2,4))[4]
female_deaths = margin.table(Titanic, c(2,4))[2]
total_females = female_deaths + female_survivors
```

Lets consider the odds for survival for each gender:

```{r}
male_survival_odds = male_survivors / male_deaths
female_survival_odds = female_survivors / female_deaths
survival_odds_ratio = female_survival_odds / male_survival_odds
```

```{r echo = FALSE}
cat(survival_odds_ratio)
```

We can see from this that the a women was much more likely to survive.

The likelihood ratio of a female vs. a male surviving is: 

```{r}
male_likelihood = male_survivors / total_males
female_likelihood = female_survivors / total_females
likelihood_ratio = female_likelihood / male_likelihood
```

```{r echo = FALSE}
cat(likelihood_ratio)
```

This being the proportion of females who survived divided by the proportion of males who survived. 

__*Thus females were almost 3.5 times more likely to survive.*__

This is the langugae of log linear analysis. In this case applying a Pearson Chi-square test would reveal a highly significant interaction between these two factors _(sex and survival)_.

The underpinning of a log linear analysis is based on the __likelihood ratio chi square__. The advantage of this is that the likelihood ratio chi squares are additive, meaning we can the chi squares derived from simpler models can be added together to produce the chi squares derived from more complex models.

We can perform a likelihood ratio chi square test for a our two way table above _(although this would not be the normal way to analyse a two way table)_.

```{r echo = FALSE}
likelihood.test = function(x) {
nrows = dim(x)[1]                      # no. of rows in contingency table
ncols = dim(x)[2]                      # no. of cols in contingency table
chi.out = chisq.test(x,correct=F)      # do a Pearson chi square test
table = chi.out[[6]]                   # get the OFs
ratios = chi.out[[6]]/chi.out[[7]]     # calculate OF/EF ratios
sum = 0                                # storage for the test statistic
for (i in 1:nrows) {
     for (j in 1:ncols) {
          sum = sum + table[i,j]*log(ratios[i,j])
     }
}
sum = 2 * sum                          # the likelihood ratio chi square
df = chi.out[[2]]                      # degrees of freedom
p = 1 - pchisq(sum,df)                 # p-value
out = c(sum, df, p, chi.out[[1]])      # the output vector
names(out) = c("LR-chisq","df","p-value","Pears-chisq")
round(out,4)                           # done!
}
```

```{r}
sex.survived = margin.table(Titanic, c(2,4))   # create the contingency table
likelihood.test(sex.survived)
```

Here we see a p value of 0.000, thus we can confirm our ealier assertion that women are more likely to survive, thus rejecting the null hypothersis. However is we define a data table to be:
```{r echo=FALSE}
table <- matrix(c(1364,367,1364,366),ncol=2,byrow=TRUE)
colnames(table) <- c("Died","Survived")
rownames(table) <- c("gA","gB")
table
```

We can see that here where the there is virtually no difference between the outcomes of the two groups then the null hypothesis can be accepted and we can assert that the interaction between these two factors is not significant:
```{r}
likelihood.test(table)
```

***

##The Log Linear Analysis

```{r}
library("MASS")
```

Firstly we can conduct the same analysis we did before on a two way table simply by doing:
```{r}
loglm( ~ Sex + Survived, data = sex.survived)
```

We've passed in the contingency table and specified the variables to be looked at. We can check back and see that this has achieved the same results as out likelihood test.

Now lets include all the variables in the model.
```{r}
loglm( ~ Class + Sex + Age + Survived, data = Titanic)
```

This is a four-way chi-squared test of independance, the p value allows us to reject the null hypothesis thus indicating somewhere in our data there are factors that are interacting with one another to produce the observed cell frequencies.

Here we should introduce the term __saturated model__: A model is saturated when it includes all effects for each factor thus including all possible interactions between them. As such a model would explain the cell frequencies perfectly, it would have chi squared statistic of zero on zero degrees of freedom. This being the case it would have no explanatory power at all. Here is an exmaple of such a model:

```{r}
loglm( ~ Class * Sex * Age * Survived, data = Titanic)
```


We can remove particular interactions from our model by doing this to remove the fourway interaction:
```{r}
loglm( ~ Class * Sex * Age * Survived - Class:Sex:Age:Survived, data = Titanic)
```

Therefore this shows us that the four way interaction in the model was expendable.

The next model however with the simple factors plus the interaction between Age and Survived does predict that expected frequencies are significantly difference from the observed frequencies.

```{r}
loglm( ~ Class + Sex + Age + Survived + Age:Survived, data = Titanic)
```

***

##Testing a Specific Hypothesis

Lets say we set out with thr hypothersis that gender was related to survival on the Titanic. The two way chi-square test we have done supports this. But by the same method sex is strongly related to class and age, which both in turn seem to be strongly related to survived. Thus we need to understand class and ages part in the relationship between sex and survived.

Log linear analysis will allow us to tease apart these effects.

***

If we remove all interaction terms that involve both sex and survived and the model still fits the obseved frequencies adequately, then we can conclude that gender and survival were unrelated. So:
```{r}
sat.model = loglm(~ Class * Sex * Age * Survived, data=Titanic)
model2 = update(sat.model, ~.-(Class:Sex:Age:Survived + Sex:Age:Survived + Class:Sex:Survived + Sex:Survived))
model2
```

As we can see this leave us with a model where the expected frequencies differ significantly from the obsevered frequencies. This we are able to assert that there is an interaction between the two variables.

Lets try another model:

```{r}
model3 = update(sat.model, ~.-(Class:Sex:Age:Survived + Sex:Age:Survived + Class:Sex:Survived))
model3
```

This model also has to be rejected.

Is this getting tedious - fear not, now we'll tell you the quick way...

***

## The __step()__ function

Here's how we automate:

```{r}
step(sat.model, direction="backward")
```


The result here shows us that the most parsimonious model as indicated by the AIC (Akaike's Information Criterion). R has identifies this model by removing the interactions bewteen the four way interaction and the Sex:Age:Survived interaction.

This it appears the relationship between Sex and Survived is contioned on class...

This shows us that the relationship between Sex and Survived is conditioned on class. We can view the tables in this arrangment:
```{r}
margin.table(Titanic, c(2,4,1))
```

The odds ratios for these tables being:
```{r}
### odds ratio 1st class
(141/4) / (62/118)
### odds ratio 2nd class
(93/13) / (25/154)
### odds ratio 3rd class
(90/106) / (88/422)
### odds ratio crew
(20/3) / (192/670)
```

In all classes the odds of a female surviving were better than the odds of a male suriviving, although this varies significantly. __? Thus we can say that class has a meaningful effect on survival ?__

## Getting more information from the model...

Lets store the model in a data object:

```{r}
loglm(formula = ~Class + Sex + Age + Survived + Class:Sex + Class:Age + 
    Sex:Age + Class:Survived + Sex:Survived + Age:Survived + 
    Class:Sex:Age + Class:Sex:Survived + Class:Age:Survived, 
    data = Titanic, evaluate = FALSE) -> step.model
```

We can view the model's expected frequencies:

```{r}
fitted(step.model)
```

_We should note that our EFs do contain zeros and so there is a danger of our model being inaccurate on account of this.

We can view the model's standardised residuals:

```{r}
resid(step.model)
```

##Using the glm() function for Log Linear Modelling

To use a glm we need  data frame rather than a contingency table. We can create this by doing:

```{r}
ti = as.data.frame(Titanic)
ti
```

Isn't this form of the data so much more sensible!!!

With this data we can perform the Log Linear Analysis (here on the saturated model). Then we can use an extractor function (in this case ANOVA) to identify the importance of the different interactions


```{r}
glm.model = glm(Freq ~ Class * Age * Sex * Survived, data = ti, family = poisson)
anova(glm.model, test = "Chisq")
```

Based on the output we can see that the interactions that we want to investigate for possible elimination are: Class:Age:Sex, Age:Sex:Survived and Class:Age:Sex:Survived. This is due to the p values on these rows indivating that the addition of these factors to the model gave no significant benifit to 'reducing deviance' between the Efs and the observed frequencies.

```{r}
anova(update(glm.model, .~.-(Class:Age:Sex:Survived + Age:Sex:Survived + Class:Age:Sex)), test = "Chisq")
```

From this we can calculate the p value by taking the final Redidual Deviation and the Degrees of freedom:
```{r}
1- pchisq(22, df=7)
```

Based on the p value we can see due to it's small size that it is not a good model (note how the step() function before retained the Class:Age:Sex interaction). We need to put one of these interactions back in - to get a model that is acceptable.

Viewing the results of a log linear analysis as a mosaic plot can also be helpful:
```{r}
mosaicplot(Titanic, shade = T)
```







